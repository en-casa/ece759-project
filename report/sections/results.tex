
The table below illustrates the lowest classification errors we were able to achieve with our Decision Tree and LDA classifiers on the MNIST and Yale Extended B datasets. 

\begin{table}[H]
  \centering
  \begin{tabular}{||c | c | c||} 
    \hline
    Algorithm & MNIST & Yale B \\
    \hline\hline
    LDA &  &  \\ 
    \hline
    Decision Tree & 18.42\% & 81.16\% \\ 
    \hline
  \end{tabular}
  \caption{Lowest classification errors achieved with LDA and Decision Trees.}
\end{table}

\subsection{Preliminary Hyperparameter Optimization}

\subsubsection{Decision Trees}

The table below illustrates some results that we obtained during various configurations of the decision tree training algorithm. In an effort to optimize training time vs. test accuracy, we noticed some trends in the algorithm's performance. Notably, utilizing \code{minLeaf} doesn't seem to contribute to over-fitting, as was noted in other implementations \cite{matlab:fitctree}. This may be in part to the type of algorithm we are using. Particularly, the pure C4.5 algorithm moves back up the tree if certain conditions are met, thereby improving a previously-made split. This bore some extra complexity than we were able to achieve at this time, but may be responsible for a more simpler tree generation on our part.

\begin{table}[H]
  \centering
  \begin{tabular}{||c | c | c | c | c | c | c | c | c | c | c | c||} 
    \hline
    Training Configuration & 1 & 2 & 2 & 2 & 3 & 3 & 1 & 1 & 1 & 1 & 4 \\
    \hline
    numFeatures & 30 & 30 & 60 & 200 & 200 & 200 & 30 & 100 & 100 &  20 & 10 \\
    \hline
    minLeaf & 1 & 1 & 2 & 2 & 3 & 4 & 4 & 4 & 1 & 1 & 1 \\
    \hline
    Error Rate & 22 & 26 & 26 & 26 & 23 & 24 & 22.6 & 23.6 & 23.5 & 22.9 & 18.42 \\
    \hline
    Mins To Train & 48 & 2 & 2 & 3 & 12 & 12 & 47 & 60 & 102 & 11 & 3 \\
    \hline
  \end{tabular}
  \caption{Hyperparameter Configuration of Decision Tree.}
\end{table}

The training configurations were primarily modifications to the algorithm's search through the feature's to find the best one to split the set on. They are as follows:

%
\begin{enumerate}
\item All features were considered for the best information gain in a particular set.
\item The decision tree was grown by considering only the first feature in the set. This was an attempt to reduce the computational complexity.
\item If the number of features left in a set was greater than 5, we considered only the first 5. Otherwise, we considered all features left. 
\item For configurations 1-3, we utilized features generated via PCA. For this experiment, we utilized features generated with LDA. This yielded our best performance.
\end{enumerate}