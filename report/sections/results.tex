
The table below illustrates the lowest classification errors we were able to achieve with our Decision Tree and LDA classifiers on the MNIST and Yale Extended B datasets. 

\begin{center}
 \begin{tabular}{||c | c | c||} 
   \hline
   Algorithm & MNIST & Yale B \\
   \hline\hline
   LDA &  &  \\ 
   \hline
   Decision Tree &  &  \\ 
   \hline
\end{tabular}
\end{center}

\subsection{Preliminary Hyperparameter Optimization}

\subsubsection{Decision Trees}

The table below illustrates some results that we obtained during various configurations of the decision tree training algorithm. In an effort to optimize training time vs. test accuracy, we noticed some trends in the algorithm's performance. Notably, utilizing \code{minLeaf} doesn't seem to contribute to over-fitting, as was noted in other implementations \cite{matlab:fitctree}. 


\begin{center}
 \begin{tabular}{||c | c | c | c | c | c | c | c | c | c | c | c||} 
   \hline
   Training Configuration & 1 & 2 & 2 & 2 & 3 & 3 & 1 & 1 & 1 & 1 & 4 \\
   \hline
   numFeatures & 30 & 30 & 60 & 200 & 200 & 200 & 30 & 100 & 100 &  20 & 10 \\
   \hline
   minLeaf & 1 & 1 & 2 & 2 & 3 & 4 & 4 & 4 & 1 & 1 & 1 \\
   \hline
   Error Rate & 22 & 26 & 26 & 26 & 23 & 24 & 22.6 & 23.6 & 23.5 & 22.9 & 18.42 \\
   \hline
   Mins To Train & 48 & 2 & 2 & 3 & 12 & 12 & 47 & 60 & 102 & 11 & 3 \\
   \hline
\end{tabular}
\end{center}


