\documentclass[]{../ncmathy}

\begin{document}
LDA relies on the assumption of normal distribution of data for each class. Linear discriminant analysis frequently achieves good performances in the tasks of face and object recognition, even though the assumptions of common covariance matrix among groups and normality are often violated (Duda, et al., 2001) (Tao Li, et al., 2006)'.

Since MNIST and Yale datasets are high-dimensional, we can not check the normality of variables. Instead, we can reduce the dimensions via a projection.

We need to employ multiclass LDA as we have 10 classes. The class separation in this case will be given by the ratio of $\frac{\pmb w^T \Sigma_b \pmb w}{\pmb w \Sigma \pmb w}$. In the case of two classes, this will reduce just to the ratio of between-class variance to within-class variance.

The steps that we follow in our LDA algorithm are: 
\begin{enumerate}
	\item We calculate within-class scatter matrix $\Sigma_i = \frac{1}{N_{i}-1} \sum\limits_{\pmb x \in D_i}^n (\pmb x - \pmb m_i)\;(\pmb x - \pmb m_i)^T$ for each class, then sum them up to get $\Sigma_W = \sum\limits_{i =1}^{c} (N_i - 1) \Sigma_i  $.
	\item We also calculate the between-class scatter matrix by $S_B = \sum\limits_{i =1}^c N_i (\pmb \mu_i - \pmb \mu) (\pmb \mu_i - \mu^T$.
	\item We need to find eigenvectors and eigenvalues of $\Sigma^{-1} \Sigma_b$.
	\item We then sort the eigenvectors depending on the magnitude of eigenvalues.
	\item The number of linear discriminants will be $c-1$ which will be 9. We need to verify that.
	\item Project our data onto the subspace(constructed by the eigenvectors of the highest eigenvalues).
\end{enumerate}

\end{document}
