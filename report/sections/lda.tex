\subsection{Linear Discriminant Analysis (LDA)}

Our classification success criterion is the extent to which the error rate is minimized. For LDA, we employ the Bayes Rule to classify our test points after training the classifier. We assign the test point to the class with the highest conditional probability (\textit{i.e.} $P(w_i|x)$, where $w_i$ is the class and $x$ the test vector). In practice, it is not feasible to get conditional probability for a given point unless we have a large amount of data. So we estimate the distribution and calculate the probabilities from there. 

LDA relies on the assumption of a normal distribution for each class. Linear discriminant analysis generally achieves good performance in the tasks of face and object recognition, even though the assumptions of common covariance matrix among groups and normality are often violated \cite{Li2006}.

Since the MNIST and Yale datasets are high-dimensional, we cannot check the normality of individual pixels. Instead, we reduce the dimensions via orthogonal projection.
To do this, we employ multiclass LDA, as we have several classes (10 for MNIST and 38 for Yale B.) The class separation in a direction $\pmb w$ in this case is given by the ratio \cite{wiki:LDA}.
%
\begin{equation}
  S = \frac{\pmb w^T \Sigma_b \pmb w}{\pmb w \Sigma \pmb w}  
\end{equation}

In the case of two classes, this reduces to the ratio of between-class variance and within-class variance.

\subsubsection{Algorithm steps}
Below, we describe the steps necessary to construct the LDA classifier. We begin by instantiating some variables with notation:\\
\textit{\textbf{n}} is the number of classes. \\ \textit{\textbf{N}} is the total number of training data samples. \\ $\pmb N_i$ is the number of points in each class. \\ $\pmb \mu_i$ and $\pmb \mu$ are mean vectors for each class and global mean for the data.

The steps that we follow in our LDA algorithm are as follows.
 
\begin{enumerate}
	
	\item We calculate within-class scatter matrix for each class
          %
          \begin{equation}
            \Sigma_i = \frac{1}{N_{i}-1} \sum\limits_{\pmb x \in D_i}^n (\pmb x - \pmb \mu_i)\;(\pmb x - \pmb \mu_i)^T
          \end{equation} 
          then sum them to obtain
          %
          \begin{equation}
             \Sigma_W = \sum\limits_{i =1}^{n} (N_i - 1) \Sigma_i 
          \end{equation}
	\item We then find the average within-class scatter matrix by calculating 
          %
          \begin{equation}
            \Sigma =\dfrac{\Sigma_W}{N}
          \end{equation}
	\item We also calculate the between-class scatter matrix by 
          %
          \begin{equation}
            \Sigma_B = \sum\limits_{i =1}^n \dfrac{N_i}{N} (\pmb \mu_i - \pmb \mu) (\pmb \mu_i - \pmb \mu)^T
          \end{equation}
	\item We need to find eigenvectors and eigenvalues of $\Sigma^{-1} \Sigma_b$.
	\item We then sort the eigenvectors depending on the magnitude of eigenvalues.
	\item The number of highest eigenvalues will be $c-1$ which will be 9 and 37 respectively for MNIST and Extended Yale datasets.
	\item Project our data onto the subspace(constructed by the eigenvectors of the highest eigenvalues).
	\item Since we have reduced dimensions of our data, we can easily apply a discriminant function for a test vector to see which class it belongs
	\item The discriminant function for each class is 
          %
          \begin{equation}
            f_i(x_k) = \mu_i {w_a}^{-1} {x_k}^T - \dfrac{1}{2} \mu_i {w_a}^{-1} {\mu_i}^T + ln(P_i)
          \end{equation} 
          where $P_i$ is the probability of each class.
	\item Then we select the class with highest discriminant function evaluation.
\end{enumerate}

\subsubsection{Helpful Functions}

h the above pseudo code, we used own created functions. After we do steps 1-7, there is a function \code{classify\_comparison} which applies discriminant function in order to classify the test data. I also tried to calculate if I get good results using the distances from the centers of each classes using \code{classify\_from\_centroid}, although it did not work very well on MNIST dataset it does perform well with Extended Yale datasets.

\subsubsection{Derivation of discriminant function}
We classify the data point as being in class $\pmb i$ if
\(P(\pmb x|i) P(i) > P(\pmb x|j) P(j)\), for $\forall j\neq i$. 
Assuming all covariances are equal $\Sigma = \Sigma_i = \Sigma_j$ and they follow a Multivariate Normal distribution formula $P(\pmb x| i) = \dfrac{1}{(2 \pi)^{n/2} |\Sigma_i|^{1/2}} exp(-\frac{1}{2} (\pmb x - \pmb \mu_i)^T \Sigma_i^{-1} (\pmb x - \pmb \mu_i))$ the above condition becomes
\(
\dfrac{P(i)}{(2 \pi)^{n/2} |\Sigma_i|^{1/2}} exp(-\frac{1}{2} (\pmb x - \pmb \mu_i)^T \Sigma^{-1} (\pmb x - \pmb \mu_i) ) > \dfrac{P(j)}{(2 \pi)^{n/2} |\Sigma|^{1/2}} exp(-\frac{1}{2} (\pmb x - \pmb \mu_j)^T \Sigma^{-1} (\pmb x - \pmb \mu_j) )
\) 
	Taking the logarithm of both sides we get 
	\(
	ln(|\Sigma|) - 2 ln(P(i)) + (\pmb x - \pmb \mu_i)^T \Sigma^{-1} (\pmb x - \pmb \mu_i) < ln(|\Sigma|) - 2 ln(P(j)) + (\pmb x - \pmb \mu_j)^T \Sigma^{-1} (\pmb x - \pmb \mu_j)
	\)
	After some algebraic manipulation, we have 
		\(
		ln(P(i)) + \pmb \mu_i \Sigma^{-1} \pmb x^T - \frac{1}{2} \pmb \mu_i \Sigma^{-1} \pmb \mu_i^T > ln(P(j)) + \pmb \mu_j \Sigma^{-1} \pmb x^T - \frac{1}{2} \pmb \mu_j \Sigma^{-1} \pmb \mu_j^T    
		\)
		Let $f_i =  \pmb \mu_i \Sigma^{-1} \pmb x^T - \frac{1}{2} \pmb \mu_i \Sigma^{-1} \pmb \mu_i^T + ln(P(i))$, then we can write 
		\(
		f_i > f_j, \forall j\neq i
		\)