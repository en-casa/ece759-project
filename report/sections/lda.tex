\subsection{Linear Discriminant Analysis}

Our classification criterion is to misclassify as small as possible. The rule we employ in classifying the data points is through the use of Bayes Rule. We put the data point to the group with the highest conditional probability(i.e. $P(w_i|x)$). In practice, it is not feasible to get conditional probability for a given point unless we have a huge data. So we should assume the distribution and calculate the probabilities. 

LDA relies on the assumption of normal distribution of data for each class. Linear discriminant analysis frequently achieves good performances in the tasks of face and object recognition, even though the assumptions of common covariance matrix among groups and normality are often violated (Duda, et al., 2001) (Tao Li, et al., 2006)'.

Since MNIST and Yale datasets are high-dimensional, we can not check the normality of variables. Instead, we can reduce the dimensions via a projection.
We need to employ multiclass LDA as we have several classes. The class separation in this case will be given by the ratio of $\frac{\pmb w^T \Sigma_b \pmb w}{\pmb w \Sigma \pmb w}$. In the case of two classes, this will reduce just to the ratio of between-class variance to within-class variance.

The steps that we follow in our LDA algorithm are: 
\begin{enumerate}
	\item We calculate within-class scatter matrix $\Sigma_i = \frac{1}{N_{i}-1} \sum\limits_{\pmb x \in D_i}^n (\pmb x - \pmb \mu_i)\;(\pmb x - \pmb \mu_i)^T$ for each class, then sum them up to get $\Sigma_W = \sum\limits_{i =1}^{c} (N_i - 1) \Sigma_i  $.
	\item Then we find average within-class scatter matrix by calculating $w_a =\dfrac{\Sigma_W}{\# of classes}$
	\item We also calculate the between-class scatter matrix by $S_B = \sum\limits_{i =1}^c N_i (\pmb \mu_i - \pmb \mu) (\pmb \mu_i - \pmb \mu)^T$.
	\item We need to find eigenvectors and eigenvalues of $\Sigma^{-1} \Sigma_b$.
	\item We then sort the eigenvectors depending on the magnitude of eigenvalues.
	\item The number of linear discriminants will be $c-1$ which will be 9. We need to verify that.
	\item Project our data onto the subspace(constructed by the eigenvectors of the highest eigenvalues).
	\item Since we have reduced dimension of our data, we can easily apply a discriminant function for a test vector to see which class it belongs
	\item A discriminant function for each class is $f(i) = \mu_i {w_a}^{-1} {x_k}^T - \dfrac{1}{2} \mu_i {w_a}^{-1} {\mu_i}^T + ln(P_i)$ where $P_i$ is respective probabilities of each class.
	\item Then we select a class with highest discriminant function.
\end{enumerate}
\subsubsection{Helpful Functions}
To accomplish the above pseudo code, we used own created functions.  
