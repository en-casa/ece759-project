\subsection{Linear Discriminant Analysis}

Our classification criterion is to misclassify as small as possible. The rule we employ in classifying the data points is through use of Bayes Rule. We put the data point to the group with the highest conditional probability. In practice, it is not feasible to get conditional probability for a given point unless we have a huge data. So we should assume the distribution and calculate the probabilities. 

LDA relies on the assumption of normal distribution of data for each class. Linear discriminant analysis frequently achieves good performances in the tasks of face and object recognition, even though the assumptions of common covariance matrix among groups and normality are often violated (Duda, et al., 2001) (Tao Li, et al., 2006)'.

Since MNIST and Yale datasets are high-dimensional, we can not check the normality of variables. Instead, we can reduce the dimensions via a projection.
We need to employ multiclass LDA as we have 10 classes. The class separation in this case will be given by the ratio of $\frac{\pmb w^T \Sigma_b \pmb w}{\pmb w \Sigma \pmb w}$. In the case of two classes, this will reduce just to the ratio of between-class variance to within-class variance.

The steps that we follow in our LDA algorithm are: 
\begin{enumerate}
	\item We calculate within-class scatter matrix $\Sigma_i = \frac{1}{N_{i}-1} \sum\limits_{\pmb x \in D_i}^n (\pmb x - \pmb \mu_i)\;(\pmb x - \pmb \mu_i)^T$ for each class, then sum them up to get $\Sigma_W = \sum\limits_{i =1}^{c} (N_i - 1) \Sigma_i  $.
	\item We also calculate the between-class scatter matrix by $S_B = \sum\limits_{i =1}^c N_i (\pmb \mu_i - \pmb \mu) (\pmb \mu_i - \pmb \mu)^T$.
	\item We need to find eigenvectors and eigenvalues of $\Sigma^{-1} \Sigma_b$.
	\item We then sort the eigenvectors depending on the magnitude of eigenvalues.
	\item The number of linear discriminants will be $c-1$ which will be 9. We need to verify that.
	\item Project our data onto the subspace(constructed by the eigenvectors of the highest eigenvalues).
	\item Since we have a reduced dimension for our data, we can easily apply nearest neighbors method in order to get the classes for our test data
	\item 
\end{enumerate}
